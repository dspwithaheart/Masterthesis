\chapter{Design and Implementation}
\label{Implementation}

This chapter describes the Design process and the details of the realised implementation of a Docker based application developed for managing ROS2 Nodes in a local Network over a web interface. Figure \ref*{fig:Background:LifecycleManagement} provides an overview of the Structure of the developed Application. The Application is supposed to have control over all the compatible ROS2 Applications running within a defined Network. It utilizes the ROS2 DDS System (for communication within same LAN) and ROS2 Routing Service (for communication between different Networks). It consists of a customized ROS2 based Docker Containers within which individual ROS2 Apps run and another customized Docker Container running a NodeJS based Web-Application (Lifecycle Management Dashboard).   

\begin{figure}[H]
	\centering
	\includegraphics[width=0.95\textwidth]{"Bilder/Application-Structure.png"}
	\caption{Structure of Lifecycle Management Application}
	\label{fig:Background:LifecycleManagement}					
\end{figure}
The application consists of following major parts:

\begin{itemize}
	\item The Docker environment
	\item ROS2 Lifecycle Application
	\item Vue based Webinterface
	\item Integration
\end{itemize}

They will be explained in detail in the following section.

\section{The Docker environment}
\label{Implementierung:DockerEnvironment}
In order to develop an Industrial Edge Application, its necessary to pack an application into a Docker Image. Then it can be converted to an IE App using the Industrial Edge App Publisher.

This section describes the Docker image developed for the purposes of this thesis. A Dockerfile is used to define a Docker image. It consists of all the necessary software libraries and third party applications and precise order in which they have to installed. The following section will describe the details of the Dockerfile(structre of the Docker image) step by step.
\subsection{The Dockerfile}
\paragraph{Settng up the Base-Image} This is the first line in any Dockerfile. In this line the Ubuntu Ros Image to use is specified.
\begin{lstlisting}[language=docker,
	% caption={Dockerfile}, 
	label={code:DockerTestumgebung}]
	FROM ros:foxy-ros-core-focal
\end{lstlisting}


\paragraph{Installing Bootstrap Tools} This step installs the Bootstrap Tools(apt-utils, build-essentials, nano, git, curl, tmux) and necessary python3 extensions (python3-colcon-common-extensions, python3-colcon-mixin, python3-rosdep, python3-vcstool)

\begin{lstlisting}[language=docker,
	% caption={Dockerfile}, 
	label={code:DockerTestumgebung}]
	# install bootstrap tools
	RUN apt-get update && apt-get install -y --no-install-recommends \
		apt-utils \
		build-essential \
		nano \
		git \
		curl \
		tmux \
		inetutils-ping \
		python3-colcon-common-extensions \
		python3-colcon-mixin \
		python3-rosdep \
		python3-vcstool \
		&& rm -rf /var/lib/apt/lists/*

\end{lstlisting}

\paragraph{Setting up Ros Build environment} The rosdep is initialized and updated before installing any ROS2 Package. Then colcon mixin and metadata are setup and any relataed libraries are updated. Colcon is the default and recommended build system for ROS2. Now ROS2 applications can be built using colcon in the next step.
\begin{lstlisting}[language=docker,
	% caption={Dockerfile}, 
	label={code:DockerTestumgebung}]
	# bootstrap rosdep
	RUN rosdep init && \
	rosdep update --rosdistro $ROS_DISTRO
	# setup colcon mixin and metadata
	RUN colcon mixin add default \
		https://raw.githubusercontent.com/colcon/
		colcon-mixin-repository/master/index.yaml && \
		colcon mixin update && \
		colcon metadata add default \
		https://raw.githubusercontent.com/colcon/
		colcon-metadata-repository/master/index.yaml && \
		colcon metadata update
\end{lstlisting}

\paragraph*{Installimg necessary Ros packages} Following ROS2 Packages are instllen in this step: ros-foxy-ros-base=0.9.2-1*, ros-foxy-joy, ros-foxy-diagnostic-updater,ros-foxy-lifecycle. Depending on the type of ROS2 Appliation it might be necessary to install further Packages, which should be done in this to avoid furthr complications. Finally the system is updated to fix any broken installs and install any mising libraries.
\begin{lstlisting}[language=docker,
	% caption={Dockerfile}, 
	label={code:DockerTestumgebung}]
	# install ros2 packages
	RUN apt-get update && apt-get install -y --no-install-recommends \
		ros-foxy-ros-base=0.9.2-1* \
		ros-foxy-joy \
		ros-foxy-diagnostic-updater \
		# install for lifecycle 
		ros-foxy-lifecycle \
		nginx \
		python \
	# update system
		&& apt-get upgrade -y \
		&& rm -rf /var/lib/apt/lists/* 
\end{lstlisting}


\paragraph*{Sourcing Ros2} Sourcing is a common term among UNIX users. In order to make the desired changes applicable to the current shell , it is necessary to "source" the file. Furthermore adding sources to .bashrc files preconfigures any new shell opened. In this step ROS2 setup files are sourced so that other ROS2 commands can be run from the command line. 
\begin{lstlisting}[language=docker,
	% caption={Dockerfile}, 
	label={code:DockerTestumgebung}]
	#add sources to bashrc
	RUN echo "source /opt/ros/foxy/setup.bash" >> ~/.bashrc \
		&& echo "source /root/colcon_ws/install/setup.bash" >> ~/.bashrc
\end{lstlisting}

\paragraph*{Create and build Ros Workspace} In this step first of all the build workspaceis setup, then the necessary source files are copied from external working directory to the Docker internal workspace and finally built using colcon.
\begin{lstlisting}[language=docker,
	% caption={Dockerfile}, 
	label={code:DockerTestumgebung}]
	# create and build workspace
	ENV ROS2_WS /root/colcon_ws
		
	COPY ./evo_siemensrob_ctrl ${ROS2_WS}/src/evo_siemensrob_ctrl/.
	COPY ./joy_converter ${ROS2_WS}/src/joy_converter/.
	COPY ./joytovel ${ROS2_WS}/src/joytovel/.
	COPY ./twist_mux ${ROS2_WS}/src/twist_mux/.

	RUN cd ${ROS2_WS} \
		&& . /opt/ros/foxy/setup.sh \
		&& colcon build
		
	# install RIB support
	COPY ./RIBInstall .

	RUN ./RIBInstall \
		&& rm -rf RIBInstall
\end{lstlisting}

\paragraph{Setup Node enviroment for Vue} This step installs a version of NodeJS in this Docker environment. NodeJS will be necessary to build the Vue-application and run ros2-web-bridge in forthcoming steps. 
\begin{lstlisting}[language=docker,
	% caption={Dockerfile}, 
	label={code:DockerTestumgebung}]
	# For Lifecycle-Dashboard Vue App
	ENV NODE_VERSION=12.6.0
	# Already installed RUN apt install -y curl
	RUN curl -o- https://raw.githubusercontent.com/
	creationix/nvm/v0.34.0/install.sh | bash
	ENV NVM_DIR=/root/.nvm
	RUN . "$NVM_DIR/nvm.sh" && nvm install ${NODE_VERSION}
	RUN . "$NVM_DIR/nvm.sh" && nvm use v${NODE_VERSION}
	RUN . "$NVM_DIR/nvm.sh" && nvm alias default v${NODE_VERSION}
	ENV PATH="/root/.nvm/versions/node/v${NODE_VERSION}/bin/
	:${PATH}"
	RUN node --version
	RUN npm --version
\end{lstlisting}



\paragraph{Copy package.json files to install Node Project dependencies} A package.json file is a standard package specification file for NodeJS applications. It contains all the necessary configurations and list of packages to successfully build a NodeJS aplication. In this step 
a working directory for the Vue Application is created, the the package.json is copied to the Docker internal workdirectory and finally the project depencencies are downloaded and installed.
\begin{lstlisting}[language=docker,
	% caption={Dockerfile}, 
	label={code:DockerTestumgebung}]
	# build stage
	WORKDIR /app
	COPY ./lifecycle-dashboard/package*.json ./
	RUN npm install
\end{lstlisting}

\paragraph*{Build the web app for production} The project source files for the "Lifecycle management application" are copied from external workspace to the Docker workdirectory and built for production.
\begin{lstlisting}[language=docker,
	% caption={Dockerfile}, 
	label={code:DockerTestumgebung}]
	# build stage
	COPY ./lifecycle-dashboard/ .
	RUN npm run build
\end{lstlisting}

\paragraph{Setting up a Nginx web server} In this step the nginx configuration file is copied from external workspace to the docker enviroment. THe built application consisting of only .html, .css and .js files are copid to a location where nginx can easily find them and use them to start the web-application. Furthermore PORT 80 is exposed and the web-application can thus be accessed on locahost:8080 within the same network where this Docker container is running.
\begin{lstlisting}[language=docker,
	% caption={Dockerfile}, 
	label={code:DockerTestumgebung}]
	# For nginx
	COPY ./nginx.conf /etc/nginx/nginx.conf
	RUN cp -a /app/dist/. /var/www/html
	EXPOSE 80
\end{lstlisting}

\paragraph*{Installing ros2-web-bridge} In this step ros2-web-bridge is installed and other services that need to launch at startup are setup. The web-server(nginx) is setup to start at launch.
\begin{lstlisting}[language=docker,
	% caption={Dockerfile}, 
	label={code:DockerTestumgebung}]
	
	# install ros2-web-bridge

	# set workdirectory as /root
	WORKDIR /root

	#for automatic launch when container gets started 
	COPY ./dockerfile_startup.sh dockerfile_startup.sh
	RUN ["chmod", "+x", "./dockerfile_startup.sh"]
	CMD ./dockerfile_startup.sh	
\end{lstlisting}

\paragraph{Starting Ros Application} This step launches the installed ROS2-Application using its launchfile.
\begin{lstlisting}[language=bash,
	% caption={dockerfile_startup}, 
	label={code:DockerTestumgebung}]
	#!/bin/bash

	nginx
	. /opt/ros/foxy/setup.sh
	. /root/colcon_ws/install/setup.sh
	npm install ros2-web-bridge
	node node_modules/ros2-web-bridge/bin/rosbridge.js &
	ros2 launch evo_siemensrob_ctrl agv_control_launch.py 
\end{lstlisting}

\paragraph{Startup and Build scripts} Hh

% \begin{lstlisting}[language=html,
% 	caption={build_lifecycle.sh}, 
% 	label={code:DockerTestumgebung}]
% 	#!/bin/bash
% 	docker build . -t ubuntu_foxy:manual_control_rti
% 	docker run -it --privileged --network host --group-add dialout --ipc="host" --name agv_control_joy_rti ubuntu_foxy:manual_control_rti
% \end{lstlisting}

\begin{lstlisting}[language=html,
	% caption={run_lifecycle.sh}, 
	label={code:DockerTestumgebung}]
	#!/bin/bash
	docker start agv_control_joy_rti

	docker exec -it agv_control_joy_rti bash 
\end{lstlisting}

\subsection{Running multiple services in a container}
The most important running process of a container is the ENTRYPOINT and/or CMD at the end of the Dockerfile. It is generally recommended to separate areas of concern by using one service per container. This service can split into several processes (e.g. the Apache webserver starts several working processes). It is fine to have multiple processes but to get the most benefit from Docker, you should avoid having one container responsible for multiple aspects of your entire application. You can connect multiple containers via custom networks and shared volumes.

The main container process is responsible for managing all the processes it launches. In some cases, the main process is not well thought out and cannot properly terminate child processes when the container is terminated (reaping). If your process falls into this category, you can use the --init option when starting the container. The --init option adds a tiny init process as the main process in the container and takes care of reaping all processes when the container is terminated. Handling such processes is better than using a full-fledged init process like sysvinit, upstart, or systemd to manage the process lifecycle inside your container.

\begin{lstlisting}[language=bash,
	% caption={dockerfile_startup}, 
	label={code:Vue}]
	#!/bin/bash
  
	# turn on bash's job control
	set -m
	
	# Start the primary process and put it in the background
	./my_main_process &
	
	# Start the helper process
	./my_helper_process
	
	# the my_helper_process might need to know how to wait on the
	# primary process to start before it does its work and returns
	
	
	# now we bring the primary process back into the foreground
	# and leave it there
	fg %1
	
\end{lstlisting}

\begin{lstlisting}[language=dockerfile,
	% caption={dockerfile_startup}, 
	label={code:Vue}]
	# syntax=docker/dockerfile:1
	FROM ubuntu:latest
	COPY my_first_process my_first_process
	COPY my_second_process my_second_process
	COPY my_wrapper_script.sh my_wrapper_script.sh
	CMD ./my_wrapper_script.sh
	
	
\end{lstlisting}

\lstinline{https://docs.docker.com/config/containers/multi-service_container/}

\section{ROS2 Lifecycle Application}
\label{Implementierung:ROS2LifecycleApplication} This section describes the details of an example ROS2-Application with managed states and steps necessary to convert a normal ROS2-node into a managed lifecycle node. 
   

\subsection{Structure of a normal ros2 publisher node vs lifecycle node}
To demonstrate the process of developing a managed lifecycle node a JoyToVel publisher node is converted into a lifecycle node in this section. The JoyToVel publisher takes input from a PlayStation4 Controller and publishes velocity and direction commands to \lstinline{\cmd_vel} topic in a ROS2 Network. Any application(node) subscribed to this topic can be controlled using this JoyToVel node.

A normal JoyToVel publisher consists of a constructor, application-specific callback functions and methods, and the main function. The necessary subscribers and publishers are initialized in the constructor. It also includes the application logic. In the main function, the node is started with the specified parameters. No changes to any parameters can be made once the node has been started. If the node crashes for some reason it has to be respawned manually. Since the JoyToVel node is an interface node, it would be beneficial to be able to dynamically reconfigure its different parameters and it demands higher reliability and stability. \\
A normal ROS2 C++ publisher node has the following structure:   
\begin{lstlisting}[language=cpp]
	#include "JoyToVel.hpp"
	namespace evo {
		//Constructor
		JoyToVel::JoyToVel() : Node("joy_to_vel")
		{
			// init subscribers
			...
			// init publishers
			...
		}

		//Callbacks and Methods
		void JoyToVel::topic_callback(const sensor_msgs::msg::Joy::SharedPtr msg) const
		{
			...
		}

		void JoyToVel::timer_callback()
		{  
			...
		}

		void JoyToVel::publishVel(const sensor_msgs::msg::Joy::SharedPtr msg)
		{
			...
		}
	}

	int main(int argc, char** argv)
	{
		rclcpp::init(argc, argv);
  
		rclcpp::spin(std::make_shared<evo::JoyToVel>());
		rclcpp::shutdown();

		return 0;
	}

\end{lstlisting}

A lifecycle based ros2 publisher has following structure:
\begin{lstlisting}[language=cpp]
namespace evo {
  class LifecycleTalker : public rclcpp_lifecycle::LifecycleNode
  {
  public:
    /// LifecycleTalker constructor
    explicit LifecycleTalker(const std::string &node_name, bool intra_process_comms = false)
        : rclcpp_lifecycle::LifecycleNode(node_name,
                                          rclcpp::NodeOptions().use_intra_process_comms(intra_process_comms))
    {
      // init subscribers
      ....
      // init publishers
      ....
    }
  
    /// Transition callback for state configuring
    rclcpp_lifecycle::node_interfaces::LifecycleNodeInterface::CallbackReturn
    on_configure(const rclcpp_lifecycle::State &)
    {
      // Initialize and configure publishers and timers.
      pub_ = this->create_publisher<std_msgs::msg::String>("lifecycle_chatter", 10);
	  timer_ = this->create_wall_timer(
          1s, std::bind(&evo::LifecycleTalker::timer_callback, this));

      return rclcpp_lifecycle::node_interfaces::LifecycleNodeInterface::CallbackReturn::SUCCESS;
    }

    /// Transition callback for state activating
    rclcpp_lifecycle::node_interfaces::LifecycleNodeInterface::CallbackReturn
    on_activate(const rclcpp_lifecycle::State &)
    {
      // Activate the lifecycle publisher.
      pub_->on_activate();

      // Important tasks in the activating phase.
	  ....
 
      return rclcpp_lifecycle::node_interfaces::LifecycleNodeInterface::CallbackReturn::SUCCESS;
    }

    /// Transition callback for state deactivating
    rclcpp_lifecycle::node_interfaces::LifecycleNodeInterface::CallbackReturn
    on_deactivate(const rclcpp_lifecycle::State &)
    {
      // Deactivate the lifecycle publisher.
      pub_->on_deactivate();

      return rclcpp_lifecycle::node_interfaces::LifecycleNodeInterface::CallbackReturn::SUCCESS;
    }

    /// Transition callback for state cleaningup
    rclcpp_lifecycle::node_interfaces::LifecycleNodeInterface::CallbackReturn
    on_cleanup(const rclcpp_lifecycle::State &)
    {
      // Release the shared pointers to the timer and publisher. 
      timer_.reset();
      pub_.reset();

      return rclcpp_lifecycle::node_interfaces::LifecycleNodeInterface::CallbackReturn::SUCCESS;
    }

    /// Transition callback for state shutting down
    rclcpp_lifecycle::node_interfaces::LifecycleNodeInterface::CallbackReturn
    on_shutdown(const rclcpp_lifecycle::State &state)
    {
      // Tasks to be performed in shutdown phase
      ....

      return rclcpp_lifecycle::node_interfaces::LifecycleNodeInterface::CallbackReturn::SUCCESS;
    }

	// Publish method for the publisher
	void publish()
    { 
      ...
    }

    
  private:
    // This is an instance of a lifecycle publisher. This lifecycle publisher
    // can be activated or deactivated based on state of the lifecycle node
    std::shared_ptr<rclcpp_lifecycle::LifecyclePublisher<std_msgs::msg::String>> pub_;

  };

  // Application specific Callbacks/Methods
    void evo::LifecycleTalker::topic_callback(const sensor_msgs::msg::Joy::SharedPtr msg) const
    {
      	....
    }

    void evo::LifecycleTalker::timer_callback()
    {
		....
    }

    void evo::LifecycleTalker::joyCallback(const sensor_msgs::msg::Joy::SharedPtr msg)
    {
		....
    }

    void evo::LifecycleTalker::publishVel(const sensor_msgs::msg::Joy::SharedPtr msg)
    {
		....
    }
}

int main(int argc, char** argv)
{
  setvbuf(stdout, NULL, _IONBF, BUFSIZ);
  rclcpp::init(argc, argv);

  rclcpp::executors::SingleThreadedExecutor exe;
  std::shared_ptr<evo::LifecycleTalker> lc_node = std::make_shared<evo::LifecycleTalker>("lifecycle_joytovel");

  exe.add_node(lc_node->get_node_base_interface());
  exe.spin();
  rclcpp::shutdown();

  return 0;
}


\end{lstlisting}


\subsection{Conversion of a node into a Lifecycle Node}
In order to convert a normal ROS2 node into a lifecycle node following steps need to be taken:

\begin{itemize}
	\item Import necessary libraries. Following libraries are necessary for a lifecycle node. In order to import these libraries the package \lstinline{rclcpp_lifecycle and lifecycle_msgs} must be installed on the system. \begin{lstlisting}[language=cpp]
		#include <chrono>
		#include <thread>
		#include <utility>
		#include <numeric>

		#include "lifecycle_msgs/msg/transition.hpp"
		#include "rclcpp/rclcpp.hpp"
		#include "rclcpp/publisher.hpp"
		#include "rclcpp_lifecycle/lifecycle_node.hpp"
		#include "rclcpp_lifecycle/lifecycle_publisher.hpp"

		#include "rcutils/logging_macros.h"

	\end{lstlisting}
	\item Encapsulate the node in a class that inherits from class \lstinline{rclcpp_lifecycle::LifecycleNode} This brings in a series of callbacks that are called depending on the current state of the node. Each lifecycle node has a set of services associated with it that make it controllable from the outside.
	\item Initialize the public and private methods.
	\item Configure and implement transition callbacks listed in listing \ref{code:TransitionCallbacks}.
	\item Define appliation specific methods
	\item Perform necessary setup and initialization in main() function.  
\end{itemize}

\section{Vue based Webinterface}
\label{Implementierung:VueBasedWebinterface}
The developed web interface is supposed to be a web version of the lifecycle management CLI. 

\subsection{Web Application Requirements}

\subsection{Web Application Design}
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.95\textwidth]{"Bilder/webapp-design.png"}
		\caption{Overview of Lifecycle Management WebApp}
		\label{fig:Background:WebappDesign}					
	\end{figure}

\subsection{ROS2 Message Types/ Javascript Objects}

\subsection{Implementing Automatic State Transitions}

\subsection{Used libraries/ package.json}

\subsection{ROS2 Javascript Functions}

\subsection{Important Functions/ Code Overview}

\begin{lstlisting}[language=JavaScript,
	% caption={dockerfile_startup}, 
	label={code:Vue}]
	
	import * as ROSLIB from 'roslib';
	import { MDBBtn, MDBBtnGroup, MDBInput, MDBContainer, MDBRow, MDBCol } from "mdb-vue-ui-kit";

	export default {
	
	name: 'LifecycleManagement',
	components: {
		MDBBtn, MDBBtnGroup, MDBInput, MDBContainer, MDBRow, MDBCol
	},
	props: {
		msg: String,
	},
	data: function () {
		return {
		unsub: false,
		console_out: [],
		selected_logger_level: 20,
		input_node_name:"",
		added_nodes:["lifecycle_joytovel"],
		active_node: "lifecycle_joytovel",
		selected_transition: {},
		available_states: [],
		available_transitions: [],
		current_state: {},
		ros: new ROSLIB.Ros({
			url : 'ws://localhost:9090'
		})
		}
	},
	
	mounted: function () {
		var ros = new ROSLIB.Ros({
			url : 'ws://localhost:9090'
		});

		ros.on('connection', function() {
			console.log('Connected to websocket server.');
		});

		ros.on('error', function(error) {
			console.log('Error connecting to websocket server: ', error);
		});

		ros.on('close', function() {
			console.log('Connection to websocket server closed.');
		});

		this.update();
	},

	methods: {

		// Available Services :
		// /lifecycle_joytovel/change_state
		// /lifecycle_joytovel/describe_parameters
		// /lifecycle_joytovel/get_available_states
		// /lifecycle_joytovel/get_available_transitions
		// /lifecycle_joytovel/get_parameter_types
		// /lifecycle_joytovel/get_parameters
		// /lifecycle_joytovel/get_state
		// /lifecycle_joytovel/get_transition_graph
		// /lifecycle_joytovel/list_parameters
		// /lifecycle_joytovel/set_parameters
		// /lifecycle_joytovel/set_parameters_atomically
		clear() {
			this.available_states = [];
			this.current_state = [];
			this.available_transitions = [];
		},
		update() {
			this.srvGetCurrentState()
			this.srvGetAvailableTransitions()
			this.srvGetAvailableStates()
		},

		dynobtn(par){
		console.log(par);
		},

		add_node(){
			if (this.input_node_name !== "") {
				this.added_nodes.push(this.input_node_name);
				console.log(this.added_nodes);
			} 
		},

		set_active_node(data){
			this.active_node = data;
			this.clear();
			this.update();
		},
		
		srvGetCurrentState() {
		var lifecycleClient = new ROSLIB.Service({
			ros : this.ros,
			// name : `/lifecycle_joytovel/get_state`,
			name : `/${this.active_node}/get_state`,
			serviceType : 'lifecycle_msgs/GetState'
		});

		console.log('Calling Service');
		var request = new ROSLIB.ServiceRequest({});
		console.log('Result for service call on '
			+ lifecycleClient.name
			+ ': ');
			lifecycleClient.callService(request, (result) => {
				console.log(result);
				this.current_state = result.current_state;
			})
		
		},

		
		srvGetAvailableStates() {
		var lifecycleClient = new ROSLIB.Service({
			ros : this.ros,
			name : `/${this.active_node}/get_available_states`,
			// name : '/lifecycle_joytovel/get_available_states', 
			serviceType : 'lifecycle_msgs/GetAvailableStates'
		});

		console.log('Calling Service srvGetAvailableStates');
		var request = new ROSLIB.ServiceRequest({});

		var tempArr = [];
		lifecycleClient.callService(request, (result) => {
			console.log(result);
			result.available_states.forEach(element => {
				console.log(element);
				tempArr.push(element);
			});
			this.available_states = tempArr;
		})

		console.log("this.available_states");
		},

	srvGetAvailableTransitions() {
		var lifecycleClient = new ROSLIB.Service({
			ros : this.ros,
			name : `/${this.active_node}/get_available_transitions`,
			// name : '/lifecycle_joytovel/get_available_transitions', 
			serviceType : 'lifecycle_msgs/GetAvailableTransitions'
		});

		console.log('Calling Service srvGetAvailableTransitions');
		var request = new ROSLIB.ServiceRequest({});

		var tempArr = [];
		
		lifecycleClient.callService(request, (result) => {
			console.log(result);
			result.available_transitions.forEach(element => {
				console.log(element);
				tempArr.push(element);
			});
			this.available_transitions = tempArr;
		})

		console.log(this.available_transitions);
		},

		srvChangeState() {
		var lifecycleClient = new ROSLIB.Service({
			ros : this.ros,
			name : `/${this.active_node}/change_state`,
			// name : '/lifecycle_joytovel/change_state', 
			serviceType : 'lifecycle_msgs/ChangeState'
		});

		console.log('Calling Service change_state');
	
		var r_id = parseInt(this.selected_transition);
		console.log(r_id );

		var request = new ROSLIB.ServiceRequest({
			transition: {id: r_id, label : ""}
		});

		lifecycleClient.callService(request, function(result) {
			console.log('Result for service call on '
			+ lifecycleClient.name);
			console.log(request);
			console.log(result);
		}, function(result) {
			console.log('Error');
			console.log(result);
		});
		this.update();
		},

		subscribeToRosOut(){
		var listener = new ROSLIB.Topic({
			ros : this.ros,
			name : '/rosout',
			messageType : 'rcl_interfaces/msg/Log'
			});
			var msg = [];
			listener.subscribe((msg)=> {
			this.console_out.push(msg);
			if (this.console_out.length > 10) {
				this.console_out.shift();
			}
			console.log(msg);
			if (this.unsub) {
				listener.unsubscribe();
			}
			}) 
			console.log(msg);
		},

		mapLogLevel(level){
		if (level === 30) {
			return "WARN"
		} else {
			return "INFO"
		}

		}
	},
\end{lstlisting}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.95\textwidth]{"Bilder/lifecycle-dashboard.png"}
	\caption{Lifecycle Dashboard}
	\label{fig:Background:LifecycleDashboard}					
\end{figure}

\section{Integration}
\label{Implementierung:Integration}

\subsection{Conversion to Industrial Edge App} 
The Industrial Edge Publisher can be used to import the created docker-compose.yml file. The created images along with their various configurations are recognized by the Industrial Edge environment and automatically applied when the app is installed on the Edge Device. No further configuration is necessary for the Edge App. Once the app has been created, it can be uploaded to the IEM using the Industrial Edge Publisher, where it is available for further distribution.

\subsection{Docker Compose}
With a docker-compose.yml file, services and configurations can be specified for a Docker container. The application requires several Docker containers to communicate with each other. In addition, a volume is created to store any persistent data that might be useful for the application.

In the Services section of the docker-compose file, the Docker image in the current directory is built. The name of the created Docker image is then specified. Then with the start command a \gls{ROS2} Application with lifecycle node is run. The commands \lstinline{mem_limit: 500mb} and \lstinline{restart: on-failure} are necessary for Industrial Edge. Then the docker volume created in the previous step is mounted. The network to use is specified then the docker container is connected to the network.
\begin{lstlisting}[language=docker-compose,
		label={code:Vue}]
		networks :
 			ie-app-network :
	 			name : ie-app-network

		volumes:
			lifecycle_edge:
				name: lifecycle_edge
		
		services:
			ros2-lifecycle:
				build: ./
				image: ros2_foxy_lifecycle:1.0
				command: ros2 launch evo_siemensrob_ctrl agv_control_launch.py 
				mem_limit: 500mb
				restart: on-failure
				volumes:
					- lifecycle_edge
				networks:
					- ie-app-network		
\end{lstlisting}

		
	
\chapter{Test setup}	

To test the Lifecycle Management Interface and showcase a Use case for the Managed Lifecyle nodes a swarm of turtlesim nodes is created and various processes during their lifecycle are contorled (managed) with the help of a web based Lifecycle Management Interface. 
\subsection{Convertig Turtlesim into a Lifecycle node}

\subsection{Test Steps}
\begin{itemize}
	\item Reconfiguring the input sensitivity(in state Configure)
	\item Respawning on the Fly (transitions create, configure, activate)
	\item Shuttingdown individual Turtlesim Node (shutdown)
\end{itemize}